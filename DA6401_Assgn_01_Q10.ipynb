{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qundtj7cBXuB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-Z4GwEVLfrJQ"
      },
      "outputs": [],
      "source": [
        "# Activation Functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Define the activation_methods dictionary\n",
        "activation_methods = {\n",
        "    'relu': (relu, relu_derivative),\n",
        "    'sigmoid': (sigmoid, sigmoid_derivative),\\\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfUnLnDY0uit"
      },
      "outputs": [],
      "source": [
        "def encode_labels(y, num_labels):\n",
        "    encoded = np.zeros((len(y), num_labels))\n",
        "    encoded[np.arange(len(y)), y] = 1\n",
        "    return encoded\n",
        "\n",
        "def get_accuracy(Y_est, Y_actual):\n",
        "    predicted = np.argmax(Y_est, axis=1)\n",
        "    trueVal = np.argmax(Y_actual, axis=1)\n",
        "    return np.mean(predicted == trueVal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cPvu-YrwqYH9"
      },
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    def __init__(self, input_size, hidden_layers, output_size, act_func=\"relu\", init_method=\"Xavier\"):\n",
        "        self.layer_count = len(hidden_layers) + 1\n",
        "        self.act_func = act_func\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        layer_dims = [input_size] + hidden_layers + [output_size]\n",
        "        for i in range(self.layer_count):\n",
        "            if init_method == \"Xavier\":\n",
        "                if act_func in [\"tanh\", \"sigmoid\"]:\n",
        "                    factor = np.sqrt(2. / (layer_dims[i] + layer_dims[i+1]))\n",
        "                else:  # Defaulting to He for ReLU\n",
        "                    factor = np.sqrt(2. / layer_dims[i])\n",
        "                W = np.random.randn(layer_dims[i], layer_dims[i+1]) * factor\n",
        "            else:\n",
        "                W = np.random.randn(layer_dims[i], layer_dims[i+1]) * 0.01\n",
        "            B = np.zeros((1, layer_dims[i+1]))\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(B)\n",
        "\n",
        "    def forward_pass(self, data):\n",
        "        act_fn, _ = activation_methods[self.act_func]\n",
        "        self.intermediate_Z = []\n",
        "        self.intermediate_A = [data]\n",
        "        output = data\n",
        "        for layer in range(self.layer_count):\n",
        "            Z = output.dot(self.weights[layer]) + self.biases[layer]\n",
        "            self.intermediate_Z.append(Z)\n",
        "            if layer == self.layer_count - 1:\n",
        "                shifted_Z = Z - np.max(Z, axis=1, keepdims=True)\n",
        "                exp_values = np.exp(shifted_Z)\n",
        "                output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "            else:\n",
        "                output = act_fn(Z)\n",
        "            self.intermediate_A.append(output)\n",
        "        return output\n",
        "\n",
        "    def loss_function(self, predicted, actual, loss_type=\"cross_entropy\"):\n",
        "        samples = actual.shape[0]\n",
        "        if loss_type == \"cross_entropy\":\n",
        "            return -np.sum(actual * np.log(predicted + 1e-8)) / samples\n",
        "        elif loss_type == \"mean_squared_error\":\n",
        "            return np.sum((actual - predicted) ** 2) / (2 * samples)\n",
        "\n",
        "    def backpropagation(self, data, target, loss_type=\"cross_entropy\"):\n",
        "        batch_size = data.shape[0]\n",
        "        grad_W = [None] * self.layer_count\n",
        "        grad_B = [None] * self.layer_count\n",
        "\n",
        "        output = self.intermediate_A[-1]\n",
        "        delta = output - target if loss_type == \"cross_entropy\" else (output - target)\n",
        "\n",
        "        for layer in reversed(range(self.layer_count)):\n",
        "            if layer == self.layer_count - 1:\n",
        "                dZ = delta\n",
        "            else:\n",
        "                _, deriv_func = activation_methods[self.act_func]\n",
        "                dZ = delta * deriv_func(self.intermediate_Z[layer])\n",
        "            prev_A = self.intermediate_A[layer]\n",
        "            grad_W[layer] = prev_A.T.dot(dZ) / batch_size\n",
        "            grad_B[layer] = np.sum(dZ, axis=0, keepdims=True) / batch_size\n",
        "            if layer > 0:\n",
        "                delta = dZ.dot(self.weights[layer].T)\n",
        "        return grad_W, grad_B\n",
        "\n",
        "\n",
        "    def update_parameters(self, grad_w, grad_b, optimizer, config, cache):\n",
        "        \"\"\"\n",
        "        Updates model parameters using different optimization algorithms.\n",
        "\n",
        "        :param grad_w: List of weight gradients\n",
        "        :param grad_b: List of bias gradients\n",
        "        :param optimizer: Optimization algorithm (e.g., \"sgd\", \"momentum\", \"rmsprop\", \"adam\", \"nesterov\", \"nadam\")\n",
        "        :param config: Configuration object with hyperparameters (learning_rate, beta values, etc.)\n",
        "        :param cache: Dictionary to store past values for optimizers like momentum, RMSProp, Adam, NAG, and NAdam.\n",
        "        :return: Updated cache\n",
        "        \"\"\"\n",
        "        lr = config.learning_rate  # Learning rate\n",
        "\n",
        "        if optimizer == \"sgd\":\n",
        "            # Vanilla Stochastic Gradient Descent (SGD)\n",
        "            for i in range(self.layer_count):\n",
        "                self.weights[i] -= lr * grad_w[i]\n",
        "                self.biases[i] -= lr * grad_b[i]\n",
        "\n",
        "        elif optimizer == \"momentum\":\n",
        "            # Standard Momentum Optimization\n",
        "            beta = getattr(config, \"momentum\", 0.9)\n",
        "\n",
        "            if \"momentum_cache\" not in cache:\n",
        "                cache[\"momentum_cache\"] = {\"v_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                          \"v_b\": [np.zeros_like(b) for b in self.biases]}\n",
        "\n",
        "            for i in range(self.layer_count):\n",
        "                cache[\"momentum_cache\"][\"v_w\"][i] = beta * cache[\"momentum_cache\"][\"v_w\"][i] + (1 - beta) * grad_w[i]\n",
        "                cache[\"momentum_cache\"][\"v_b\"][i] = beta * cache[\"momentum_cache\"][\"v_b\"][i] + (1 - beta) * grad_b[i]\n",
        "\n",
        "                self.weights[i] -= lr * cache[\"momentum_cache\"][\"v_w\"][i]\n",
        "                self.biases[i] -= lr * cache[\"momentum_cache\"][\"v_b\"][i]\n",
        "\n",
        "        elif optimizer == \"nesterov\":\n",
        "            # Nesterov Accelerated Gradient (NAG)\n",
        "            beta = getattr(config, \"momentum\", 0.9)\n",
        "\n",
        "            if \"nesterov_cache\" not in cache:\n",
        "                cache[\"nesterov_cache\"] = {\"v_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                          \"v_b\": [np.zeros_like(b) for b in self.biases]}\n",
        "\n",
        "            for i in range(self.layer_count):\n",
        "                # Lookahead Step\n",
        "                v_prev_w = cache[\"nesterov_cache\"][\"v_w\"][i]\n",
        "                v_prev_b = cache[\"nesterov_cache\"][\"v_b\"][i]\n",
        "\n",
        "                cache[\"nesterov_cache\"][\"v_w\"][i] = beta * v_prev_w + (1 - beta) * grad_w[i]\n",
        "                cache[\"nesterov_cache\"][\"v_b\"][i] = beta * v_prev_b + (1 - beta) * grad_b[i]\n",
        "\n",
        "                self.weights[i] -= lr * (beta * v_prev_w + (1 - beta) * grad_w[i])\n",
        "                self.biases[i] -= lr * (beta * v_prev_b + (1 - beta) * grad_b[i])\n",
        "\n",
        "        elif optimizer == \"rmsprop\":\n",
        "            # RMSProp Optimization\n",
        "            decay_rate = getattr(config, \"decay_rate\", 0.99)\n",
        "            epsilon = getattr(config, \"epsilon\", 1e-8)\n",
        "\n",
        "            if \"rms_cache\" not in cache:\n",
        "                cache[\"rms_cache\"] = {\"s_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                      \"s_b\": [np.zeros_like(b) for b in self.biases]}\n",
        "\n",
        "            for i in range(self.layer_count):\n",
        "                cache[\"rms_cache\"][\"s_w\"][i] = decay_rate * cache[\"rms_cache\"][\"s_w\"][i] + (1 - decay_rate) * (grad_w[i] ** 2)\n",
        "                cache[\"rms_cache\"][\"s_b\"][i] = decay_rate * cache[\"rms_cache\"][\"s_b\"][i] + (1 - decay_rate) * (grad_b[i] ** 2)\n",
        "\n",
        "                self.weights[i] -= lr * grad_w[i] / (np.sqrt(cache[\"rms_cache\"][\"s_w\"][i]) + epsilon)\n",
        "                self.biases[i] -= lr * grad_b[i] / (np.sqrt(cache[\"rms_cache\"][\"s_b\"][i]) + epsilon)\n",
        "\n",
        "        elif optimizer == \"adam\":\n",
        "            # Adam Optimization (Adaptive Momentum Estimation)\n",
        "            beta1 = getattr(config, \"beta1\", 0.9)\n",
        "            beta2 = getattr(config, \"beta2\", 0.999)\n",
        "            epsilon = getattr(config, \"epsilon\", 1e-8)\n",
        "\n",
        "            if \"adam_cache\" not in cache:\n",
        "                cache[\"adam_cache\"] = {\"m_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                      \"v_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                      \"m_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                                      \"v_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                                      \"step\": 0}\n",
        "\n",
        "            cache[\"adam_cache\"][\"step\"] += 1\n",
        "            step = cache[\"adam_cache\"][\"step\"]\n",
        "\n",
        "            for i in range(self.layer_count):\n",
        "                cache[\"adam_cache\"][\"m_w\"][i] = beta1 * cache[\"adam_cache\"][\"m_w\"][i] + (1 - beta1) * grad_w[i]\n",
        "                cache[\"adam_cache\"][\"m_b\"][i] = beta1 * cache[\"adam_cache\"][\"m_b\"][i] + (1 - beta1) * grad_b[i]\n",
        "\n",
        "                cache[\"adam_cache\"][\"v_w\"][i] = beta2 * cache[\"adam_cache\"][\"v_w\"][i] + (1 - beta2) * (grad_w[i] ** 2)\n",
        "                cache[\"adam_cache\"][\"v_b\"][i] = beta2 * cache[\"adam_cache\"][\"v_b\"][i] + (1 - beta2) * (grad_b[i] ** 2)\n",
        "\n",
        "                m_w_hat = cache[\"adam_cache\"][\"m_w\"][i] / (1 - beta1 ** step)\n",
        "                v_w_hat = cache[\"adam_cache\"][\"v_w\"][i] / (1 - beta2 ** step)\n",
        "                m_b_hat = cache[\"adam_cache\"][\"m_b\"][i] / (1 - beta1 ** step)\n",
        "                v_b_hat = cache[\"adam_cache\"][\"v_b\"][i] / (1 - beta2 ** step)\n",
        "\n",
        "                self.weights[i] -= lr * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
        "                self.biases[i] -= lr * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "        elif optimizer == \"nadam\":\n",
        "            # NAdam (Nesterov-accelerated Adam)\n",
        "            beta1 = getattr(config, \"beta1\", 0.9)\n",
        "            beta2 = getattr(config, \"beta2\", 0.999)\n",
        "            epsilon = getattr(config, \"epsilon\", 1e-8)\n",
        "\n",
        "            if \"nadam_cache\" not in cache:\n",
        "                cache[\"nadam_cache\"] = {\"m_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                        \"v_w\": [np.zeros_like(w) for w in self.weights],\n",
        "                                        \"m_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                                        \"v_b\": [np.zeros_like(b) for b in self.biases],\n",
        "                                        \"step\": 0}\n",
        "\n",
        "            cache[\"nadam_cache\"][\"step\"] += 1\n",
        "            step = cache[\"nadam_cache\"][\"step\"]\n",
        "\n",
        "            mu_t = beta1 * (1 - 0.5 * (0.96 ** (step / 250)))\n",
        "            for i in range(self.layer_count):\n",
        "                cache[\"nadam_cache\"][\"m_w\"][i] = mu_t * cache[\"nadam_cache\"][\"m_w\"][i] + (1 - mu_t) * grad_w[i]\n",
        "                cache[\"nadam_cache\"][\"v_w\"][i] = beta2 * cache[\"nadam_cache\"][\"v_w\"][i] + (1 - beta2) * (grad_w[i] ** 2)\n",
        "\n",
        "                self.weights[i] -= lr * cache[\"nadam_cache\"][\"m_w\"][i] / (np.sqrt(cache[\"nadam_cache\"][\"v_w\"][i]) + epsilon)\n",
        "\n",
        "        return cache  # Return updated cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R1PeztwY0iu6"
      },
      "outputs": [],
      "source": [
        "def execute_training():\n",
        "    wandb.init()\n",
        "    cfg = wandb.config\n",
        "\n",
        "    # Set dynamic run name for easy tracking in WandB\n",
        "    wandb.run.name = f\"e_{cfg.num_epochs}_hls_{cfg.hiddennodes}_numhl_{cfg.hiddenlayers}_opt_{cfg.opt}\" \\\n",
        "                     f\"_bs_{cfg.batch_size}_init_{cfg.initializer}_ac_{cfg.activation_func}_loss_{cfg.loss}\" \\\n",
        "                     f\"_lr_{cfg.learning_rate}_wdecay_{cfg.weight_decay}\"\n",
        "\n",
        "    # Load and preprocess data\n",
        "    (train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()\n",
        "    train_X = train_X.reshape(train_X.shape[0], -1) / 255.0\n",
        "    test_X = test_X.reshape(test_X.shape[0], -1) / 255.0\n",
        "\n",
        "    num_classes = 10\n",
        "    train_y_oh = encode_labels(train_y, num_classes)\n",
        "    test_y_oh = encode_labels(test_y, num_classes)\n",
        "\n",
        "    # Split train into train and validation sets\n",
        "    val_split = int(0.9 * train_X.shape[0])\n",
        "    val_X, val_y_oh = train_X[val_split:], train_y_oh[val_split:]\n",
        "    train_X, train_y_oh = train_X[:val_split], train_y_oh[:val_split]\n",
        "\n",
        "    input_dim = train_X.shape[1]\n",
        "    hidden_arch = [cfg.hiddennodes] * cfg.hiddenlayers\n",
        "\n",
        "    # Initialize the model\n",
        "    model = NeuralNet(\n",
        "        input_size=input_dim,\n",
        "        hidden_layers=hidden_arch,\n",
        "        output_size=num_classes,\n",
        "        act_func=cfg.activation_func,\n",
        "        init_method=cfg.initializer\n",
        "    )\n",
        "\n",
        "    optimizer_states = {}\n",
        "    grad_clip_value = 1.0\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        # Shuffle data at the beginning of each epoch\n",
        "        shuffle_idx = np.random.permutation(train_X.shape[0])\n",
        "        train_X = train_X[shuffle_idx]\n",
        "        train_y_oh = train_y_oh[shuffle_idx]\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        correct_preds = 0\n",
        "        total_preds = 0\n",
        "\n",
        "        num_batches = train_X.shape[0] // cfg.batch_size\n",
        "        for batch in range(num_batches):\n",
        "            start = batch * cfg.batch_size\n",
        "            end = start + cfg.batch_size\n",
        "            X_batch = train_X[start:end]\n",
        "            y_batch = train_y_oh[start:end]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model.forward_pass(X_batch)\n",
        "            loss = model.loss_function(outputs, y_batch, loss_type=cfg.loss)\n",
        "            epoch_loss += loss\n",
        "\n",
        "            # Accuracy calculation\n",
        "            correct_preds += np.sum(np.argmax(outputs, axis=1) == np.argmax(y_batch, axis=1))\n",
        "            total_preds += X_batch.shape[0]\n",
        "\n",
        "            # Backward pass (compute gradients)\n",
        "            grad_w, grad_b = model.backpropagation(X_batch, y_batch, loss_type=cfg.loss)\n",
        "\n",
        "            # Gradient clipping\n",
        "            grad_w = [np.clip(g, -grad_clip_value, grad_clip_value) for g in grad_w]\n",
        "            grad_b = [np.clip(g, -grad_clip_value, grad_clip_value) for g in grad_b]\n",
        "\n",
        "            # Update model parameters using optimizer\n",
        "            optimizer_states = model.update_parameters(grad_w, grad_b, cfg.opt, cfg, optimizer_states)\n",
        "\n",
        "        # Average loss and training accuracy\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        train_acc = correct_preds / total_preds\n",
        "\n",
        "        # Validation phase\n",
        "        val_outputs = model.forward_pass(val_X)\n",
        "        val_loss = model.loss_function(val_outputs, val_y_oh, loss_type=cfg.loss)\n",
        "        val_acc = get_accuracy(val_outputs, val_y_oh)\n",
        "\n",
        "        # Log training and validation metrics to WandB\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc\n",
        "        })\n",
        "\n",
        "        # Display progress\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Test phase\n",
        "    test_outputs = model.forward_pass(test_X)\n",
        "    test_acc = get_accuracy(test_outputs, test_y_oh)\n",
        "\n",
        "    # Log final test accuracy\n",
        "    wandb.log({\"test_accuracy\": test_acc})\n",
        "    print(f\"Final Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo13Igzh0Xlr"
      },
      "outputs": [],
      "source": [
        "# Custom sweep configuration 1st Session\n",
        "\n",
        "sweep_config = {\n",
        "    'name': \"DA6401 ASSIGNMENT 01\",\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {'values': [4, 5, 6]},\n",
        "        'num_epochs': {'values': [5, 10]},  \n",
        "        'hiddennodes': {'values': [128, 256]},  # Larger networks\n",
        "        'learning_rate': {'values': [1e-3, 5e-4]},  # Higher learning rates\n",
        "        'initializer': {'values': [\"Xavier\"]},  # Force proper initialization\n",
        "        'batch_size': {'values': [128, 256]},\n",
        "        'opt': {'values': [\"sgd\", \"adam\", \"nesterov\", \"rmsprop\", \"momentum\", \"nadam\"]},\n",
        "        'activation_func': {'values': [\"relu\"]},  # Better for deep networks\n",
        "        'loss': {'values': [\"cross_entropy\"]},\n",
        "        'weight_decay': {'values': [0.0001, 0.001]}  # L2 regularization\n",
        "    }\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sweep_id = wandb.sweep(sweep_config, project=\"Q4\")\n",
        "    wandb.agent(sweep_id, function=execute_training, count=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs1zg39h0dGL"
      },
      "outputs": [],
      "source": [
        "sweep_config1 = {\n",
        "    'name': \"DA6401 ASSIGNMENT 01\",\n",
        "    'method': 'grid',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {'values': [6]},\n",
        "        'num_epochs': {'values': [15]},\n",
        "        'hiddennodes': {'values': [256]},\n",
        "        'learning_rate': {'values': [1e-3]},\n",
        "        'initializer': {'values': [\"Xavier\"]},\n",
        "        'batch_size': {'values': [256]},\n",
        "        'opt': {'values': [\"adam\"]},\n",
        "        'activation_func': {'values': [\"relu\"]},\n",
        "        'loss': {'values': [\"cross_entropy\"]},\n",
        "        'weight_decay': {'values': [0.001]}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config2 = {\n",
        "    'name': \"DA6401 ASSIGNMENT 01\",\n",
        "    'method': 'grid',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {'values': [6]},\n",
        "        'num_epochs': {'values': [10]},\n",
        "        'hiddennodes': {'values': [256]},\n",
        "        'learning_rate': {'values': [5e-4]},\n",
        "        'initializer': {'values': [\"Xavier\"]},\n",
        "        'batch_size': {'values': [256]},\n",
        "        'opt': {'values': [\"nadam\"]},\n",
        "        'activation_func': {'values': [\"relu\"]},\n",
        "        'loss': {'values': [\"cross_entropy\"]},\n",
        "        'weight_decay': {'values': [0.0008]}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config3 = {\n",
        "    'name': \"DA6401 ASSIGNMENT 01\",\n",
        "    'method': 'grid',\n",
        "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'hiddenlayers': {'values': [6]},\n",
        "        'num_epochs': {'values': [15]},\n",
        "        'hiddennodes': {'values': [256]},\n",
        "        'learning_rate': {'values': [1e-3]},\n",
        "        'initializer': {'values': [\"Xavier\"]},\n",
        "        'batch_size': {'values': [256]},\n",
        "        'opt': {'values': [\"rmsprop\"]},\n",
        "        'activation_func': {'values': [\"relu\"]},\n",
        "        'loss': {'values': [\"cross_entropy\"]},\n",
        "        'weight_decay': {'values': [0.001]}\n",
        "    }\n",
        "}\n",
        "# -----------------------\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    sweep_id = wandb.sweep(sweep_config1, project=\"Q4\")\n",
        "    wandb.agent(sweep_id, function=execute_training, count=1)\n",
        "\n",
        "    sweep_id = wandb.sweep(sweep_config2, project=\"Q4\")\n",
        "    wandb.agent(sweep_id, function=execute_training, count=1)\n",
        "\n",
        "    sweep_id = wandb.sweep(sweep_config3, project=\"Q4\")\n",
        "    wandb.agent(sweep_id, function=execute_training, count=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
